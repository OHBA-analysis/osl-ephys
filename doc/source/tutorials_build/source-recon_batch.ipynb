{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Group Analysis of Source-space Data\n\nIn this tutorial, we will perform a group analysis of parcellated source-space task MEG data using the Wakeman-Henson dataset. This is a public dataset consisting of 19 healthy individuals who performed a simple visual perception task. See [Wakeman & Henson (2015)](https://www.nature.com/articles/sdata20151) for more details.\n\nWe will do this on the raw time courses bandpass filtered between 3-14Hz. In other words, this will tell us about the strength of the visual evoked response band-limited to be from between 3Hz to 14Hz.\n\nThe steps are:\n\n1. Downloading the data from OSF\n2. Setup file names\n3. Coreg, Source reconstruction and parcellation \n4. Epoching\n5. First-Level GLM \n6. Group-Level GLM\n\nTo run this tutorial you will need to have osl-ephys and FSL installed, with the appropriate paths specified in your environment. See the instructions on the repo/read the docs for how to install these packages. Before running this tutorial, we recommend going through the **Soure reconstruction** and **Statistics (General Linear Modelling)** tutorials first.\n\n## 1. Downloading the raw data from OSF\n\nThe public Wakeman-Henson dataset provides MaxFiltered data. Note that the full dataset is available on [OpenNeuro](https://openneuro.org/datasets/ds000117/versions/1.0.4).\n\nThe full dataset contains 19 subjects, each with 6 sessions. To limit the amount of data that we have to handle for the tutorial, we will use only the first 3 sessions from each of the 19 subjects. This data can be downloaded from the OSF project website. \n\nLet's download the data. Note, to download the dataset you need osfclient installed. This can be installed by excuting the following code in a jupyter notebook cell:\n\n``!pip install osfclient``\n\nLet's now download the data. Note that this will be placed in your current working directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\ndef get_data(name):\n    print('Data will be in directory {}'.format(os.getcwd()))\n    \"\"\"Download a dataset from OSF.\"\"\"\n    if os.path.exists(f\"{name}\"):\n        return f\"{name} already downloaded. Skipping..\"\n    os.system(f\"osf -p zxb6c fetch SourceRecon/data/{name}.zip\") \n    os.system(f\"unzip -o {name}.zip\")\n    os.remove(f\"{name}.zip\")\n    return f\"Data downloaded to: {name}\"\n\n# Download the dataset\nget_data(\"wake_hen_group_raw\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup File Names\n\nLet's first setup the file names for the first 3 sessions from each of the subjects. \nNote that in the original publication, subjects 1,5 and 16 were excluded from analysis. We will do the same here.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport os.path as op\nfrom osl_ephys import source_recon\nimport numpy as np\n\nfsl_dir = '~/fsl'\nsource_recon.setup_fsl(fsl_dir)\n\nsubjects_dir = \"./wake_hen_group_raw\"\nout_dir = op.join(subjects_dir, \"glm\")\n\nnsubjects = 19\nnsessions = 3 # we will only use 3 of the 6 session/runs avaible from each subject\nsubjects_to_do = np.arange(0, nsubjects)\nsessions_to_do = np.arange(0, nsessions)\nsubj_sess_2exclude = np.zeros([nsubjects, nsessions]).astype(bool)\n\nsubj_sess_2exclude[0]=True\nsubj_sess_2exclude[4]=True\nsubj_sess_2exclude[15]=True\n\n\npreproc_fif_files = []\ninput_fif_files = []\nepoch_fif_files = []\nglm_model_files = []\nglm_time_files = []\nsubj_indices = []\n\nrecon_dir = op.join(subjects_dir, \"recon\")\nglm_dir = op.join(subjects_dir, \"glm\")\n\nif not os.path.isdir(glm_dir):\n    os.makedirs(glm_dir)\n\nfor sub in subjects_to_do:\n    for ses in sessions_to_do:\n        if not subj_sess_2exclude[sub, ses]:\n\n            sub_name = \"sub\" + (\"{}\".format(subjects_to_do[sub] + 1)).zfill(3)\n            ses_name = \"run_\" + (\"{}\".format(sessions_to_do[ses] + 1)).zfill(2)\n            subject = sub_name + \"_\" + ses_name\n\n            # output files\n            preproc_fif_file = op.join(\n                subjects_dir, subject + \"_meg\", subject + \"_meg_preproc_raw.fif\"\n            )\n            input_fif_file = op.join(\n                recon_dir, subject, \"sflip_parc-raw.fif\"\n            )\n            epoch_fif_file = op.join(\n                recon_dir, subject, \"epoch_sflip_parc-epo.fif\"\n            )\n            glm_model_file = op.join(\n                glm_dir, subject, \"first_level_glm_model.hdf5\"\n            )\n            glm_time_file= op.join(\n                glm_dir, subject, \"first_level_glm_model_times.npy\"\n            )\n\n            if op.exists(epoch_fif_file):\n                preproc_fif_files.append(preproc_fif_file)\n                input_fif_files.append(input_fif_file)\n                epoch_fif_files.append(epoch_fif_file)\n                glm_model_files.append(glm_model_file)\n                glm_time_files.append(glm_time_file)\n\n                # store which subject this session belongs to,\n                # this will be used to construct the group design matrix\n                subj_indices.append(sub)\n\n                glm_subj_dir = op.join(glm_dir, subject)\n                if not os.path.isdir(glm_subj_dir):\n                    os.makedirs(glm_subj_dir)\n\nprint(epoch_fif_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Coreg, Source Reconstruction and Parcellation\n\nSee the \"Single Subject Source Reconstruction\" tutorial for an explanation of the settings used here.\n\nHere we are using ``source_recon.run_src_batch`` to easily run Coreg, Source Reconstruction and Parcellation over all subjects and sessions. \n\nNote that we do not actually run this code here. For the sake of time, it has already been run for you (it is actually not possible to run this code as the necessary files have not been provided).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pre_run = True\n\nif not pre_run:\n\n    config = \"\"\"\n    source_recon:\n    - extract_fiducials_from_fif: {}\n    - compute_surfaces:\n        include_nose: false\n    - coregister:\n        use_nose: false\n        use_headshape: false\n    - forward_model:\n        model: Single Layer\n    - beamform_and_parcellate:\n        freq_range: [3, 20]\n        chantypes: [mag, grad]\n        rank: {meg: 55}\n        parcellation_file: HarvOxf-sub-Schaefer100-combined-2mm_4d_ds8.nii.gz\n        method: spatial_basis\n        orthogonalisation: None\n    \"\"\"\n\n    source_recon.run_src_batch(\n        config,\n        src_dir=recon_dir,\n        subjects=subjects,\n        preproc_files=preproc_fif_files,\n        smri_files=smri_files,\n    )\nelse:\n    print('Using pre-run results')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View Coregistration Report\n\nA coregistration report is output by the batch call, and can be viewed in a web browser.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('View a summary report by opening the following file in a web browser:\\n{}'.format(os.getcwd() + ('/wake_hen_group_raw/recon/report/summary_report.html')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Epoching\n\nWe next loop over subjects, epoching the data \n\nNote that we do not actually run this code here. For the sake of time, it has already been run for you (it is actually not possible to run this code as the necessary files have not been provided).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not pre_run:\n\n    for preproc_fif_file, sflip_parc_file, epoch_fif_file \\\n            in zip(preproc_fif_files, sflip_parc_files, epoch_fif_files):\n\n        # Parcellated data\n        raw = mne.io.read_raw(sflip_parc_file) \n        \n        # To get epoching info\n        dataset = osl_ephys.preprocessing.read_dataset(preproc_fif_file)\n        \n        epochs = mne.Epochs(\n            raw,\n            dataset[\"events\"],\n            dataset[\"event_id\"],\n            tmin=-0.2,\n            tmax=1.3,\n            baseline=(None, 0),\n        )\n\n        epochs.drop_bad(verbose=True)\n        epochs.load_data()\n        epochs.save(epoch_fif_file, overwrite=True)\nelse:\n    print('Using pre-run results')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. First-level GLM\n\n### Setup First-level Design Matrix\n\nRecall that we have 19 subjects, each with 3 sessions of data. In the experiment there are 9 different types of trials corresponding to 9 different conditions (i.e. different types of visual stimuli) that are presented on a video screen to the subject in the scanner. The 9 different conditions are:\n\n* FamousFirst\n* FamousImmediate\n* FamousLast\n* UnfamiliarFirst\n* UnfamiliarImmediate\n* UnfamiliarLast\n* ScrambledFirst\n* ScrambledImmediate\n* ScrambledLast\n\nThe *First-Level* analysis corresponds to separately modelling what is happening in the data of each session of each subject. We do this using a \"Trial-wise\" GLM, because the regressors in the design matrix of the GLM explain the variability over trials.\n\nNote that we will fit a trial-wise GLM **separately** to each:\n\n* session\n* parcel\n* time point within trial (or epoch)\n\nWe will now specify the content of the first-level design matrix using the package glmtools.\n\nNote that we specify 9 regressors, each of which is a categorical regressor that picks out those trials that correspond to each of the 9 different conditions.\n\nWe also specify 2 contrasts:\n\n* ``Faces_vs_Scrambled`` contrast, which computes the difference in the response between the conditions in which a person's face is presented versus those in which a scrambled face is presented. \n* ``Visual`` contrast, which computes a contrast that sums over all the 9 different conditions; this therefore corresponds to the average response over all conditions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import glmtools as glm\n\nDC = glm.design.DesignConfig()\nDC.add_regressor(name=\"FamousFirst\", rtype=\"Categorical\", codes=5)\nDC.add_regressor(name=\"FamousImmediate\", rtype=\"Categorical\", codes=6)\nDC.add_regressor(name=\"FamousLast\", rtype=\"Categorical\", codes=7)\nDC.add_regressor(name=\"UnfamiliarFirst\", rtype=\"Categorical\", codes=13)\nDC.add_regressor(name=\"UnfamiliarImmediate\", rtype=\"Categorical\", codes=14)\nDC.add_regressor(name=\"UnfamiliarLast\", rtype=\"Categorical\", codes=15)\nDC.add_regressor(name=\"ScrambledFirst\", rtype=\"Categorical\", codes=17)\nDC.add_regressor(name=\"ScrambledImmediate\", rtype=\"Categorical\", codes=18)\nDC.add_regressor(name=\"ScrambledLast\", rtype=\"Categorical\", codes=19)\nDC.add_contrast(\n    name=\"Faces_vs_Scrambled\",\n    values={\n        \"FamousFirst\": 1,\n        \"FamousImmediate\": 1,\n        \"FamousLast\": 1,\n        \"UnfamiliarFirst\": 1,\n        \"UnfamiliarImmediate\": 1,\n        \"UnfamiliarLast\": 1,\n        \"ScrambledFirst\": -2,\n        \"ScrambledImmediate\": -2,\n        \"ScrambledLast\": -2,\n    },\n)\nDC.add_contrast(\n    name=\"Visual\",\n    values={\n        \"FamousFirst\": 1,\n        \"FamousImmediate\": 1,\n        \"FamousLast\": 1,\n        \"UnfamiliarFirst\": 1,\n        \"UnfamiliarImmediate\": 1,\n        \"UnfamiliarLast\": 1,\n        \"ScrambledFirst\": 1,\n        \"ScrambledImmediate\": 1,\n        \"ScrambledLast\": 1,\n    },\n)\n\nprint(DC.to_yaml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each session will have a design matrix containing the 9 different regressors and the 2 contrasts on these regressors specified above. However, because the ordering of trials might be different for each session, we need to construct the regressors in a manner that is specific to each session.\n\nHence, we construct the design matrix for an example session.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mne\n\nsubj_index = 0\n\n# Load data in glmtools\nepochs = mne.read_epochs(epoch_fif_files[subj_index]) \ndata = glm.io.load_mne_epochs(epochs)\n\n# Create design matrix for this session\ndes = DC.design_from_datainfo(data.info)\n\nprint('Completed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualise the resulting first-level, trial-wise design matrix for the example session.\n\nYou should be able see that each regressor (column) is a categorical regressor picking out which trials correspond to each of the 9 different conditions. Below the design matrix you should also see the two contrasts that we will compute.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('First-level design matrix for subject {}'.format(subj_index))\nfig = des.plot_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit First-level GLM\n\nWe next loop over all sessions, constructing and fitting the first-level design matrices to each run (session) from each subject separately.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from osl_ephys import preprocessing\nimport h5py\n\nfor epoch_fif_file, glm_model_file, glm_time_file \\\n        in zip(epoch_fif_files, glm_model_files, glm_time_files):\n    \n    epochs = mne.read_epochs(epoch_fif_file) # e.g. sensor, source space, or parcellated data\n    epochs.load_data()\n\n    # Load data in glmtools\n    data = glm.io.load_mne_epochs(epochs)\n    \n    # Create design matrix for this session\n    design = DC.design_from_datainfo(data.info)\n\n    # Fit Model\n    model = glm.fit.OLSModel(design, data)\n\n    # Save fitted GLM\n    out = h5py.File(glm_model_file, \"w\")\n    design.to_hdf5(out.create_group(\"design\"))\n    model.to_hdf5(out.create_group(\"model\"))\n    out.close()\n    np.save(glm_time_file, epochs.times)\n\nprint('Completed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Group-level GLM\n\n### Group Model\n\nWe could perform a group analysis by simply computing a group average of the response over all sessions and subjects. However, it is more flexible to use a GLM at the group level too. We do this using a \"Session/subject-wise\" GLM, because the regressors in the design matrix of the group-level GLM explain the variability over session and subjects.\n\nNote that typically the same group-level design matrix is fit *separately* to each:\n\n* first-level contrast\n* parcel\n* timepoint within trial\n\nWe will now setup the group-level design matrix. We basically have one categorical regressor (it contains zeros and ones) for each subject that picks out which sessions belong to that subject. As such, in the visualisation of the design matrix below you will see 19 regressors for the 19 subjects, each indicating with a value of one which sessions belong to that subject.\n\nWe also need to setup contrasts on the regression parameters, to compute the COPEs (contrasts of parameter estimates) we are interested in. \n\nHere, we specify a contrast for each subject, which picks out the the regressor for that subject; and we have one contrast that computes a COPE proportional to the average of the response over all subjects (contrast 20 in the visualisation below - although note that this indexes from 1, whereas when we want to select this context in the python code later, it will have an index of 19, because python indexes from 0).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "groupDC = glm.design.DesignConfig()\n\n# Add subject mean regressors\nzero_values = dict()\nones_values = dict()\nfor subj_ind in set(subj_indices):\n    regressor_name=(\"Subj{}\".format(subj_ind))\n    groupDC.add_regressor(name=regressor_name, rtype=\"Categorical\", codes=subj_ind)\n    zero_values[regressor_name]=0\n    ones_values[regressor_name]=1\n\n# Add subject mean contrasts\nfor subj_ind in set(subj_indices):\n    contrast_name=(\"Subj{}\".format(subj_ind))\n    subj_values = zero_values.copy()\n    subj_values[contrast_name] = 1\n    groupDC.add_contrast(name=contrast_name,\n                    values=subj_values,\n                   )\n# Add group mean contrast\ngroupDC.add_contrast(name=\"Group mean\",\n                values=ones_values,\n               )\n\ndesign = groupDC.design_from_datainfo({'category_list':subj_indices})\n\nfig = design.plot_summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned above, we can fit this group-level design matrix *separately* to each:\n\n* first-level contrast\n* parcel\n* timepoint within trial \n\nHere, we will focus on fitting the group-level GLM to just the first-level contrast with index 1. This corresponds to the average over all conditions (and therefore over all trials). \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "first_level_contrast = 1 # indexing starts from 0\n\nprint(DC.contrast_names[first_level_contrast])\nprint(DC.contrasts[first_level_contrast])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output shows that the COPE this first-level contrast computes, is proportional to the average over all conditions (and therefore over all trials). In other words, we can use this first-level contrast to get the response averaged over all trials in each session. \n\n### Load First-level COPES\n\nWe start by loading in the first-level COPEs from the first-level GLM that we fit earlier, and concatenate them into a (sessions x parcels x tpts_within_trial) array.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom anamnesis import obj_from_hdf5file\n\nrectify = True\nbaseline_correct = True\nbaseline_window = (-0.2, 0) # secs\n\nprint(\"Loading first-level GLMs and extracting COPEs for first-level contrast {}\".format(first_level_contrast))\n\ndata = []\nfor glm_time_file, glm_model_file in zip(glm_time_files, glm_model_files):\n\n    # Load GLM\n    model = obj_from_hdf5file(glm_model_file, \"model\")\n    epochs_times = np.load(glm_time_file)\n\n    baseline_time_inds = np.where((epochs_times>baseline_window[0]) & (epochs_times<baseline_window[1]))[0]\n\n    cope = model.copes[first_level_contrast, :, :]\n\n    if rectify:\n        cope = abs(cope)\n\n    if baseline_correct:\n        baseline_mean = np.mean(\n            cope[:, baseline_time_inds],\n            axis=1,\n        )\n        cope = cope - np.reshape(baseline_mean, [-1, 1])\n\n    data.append(cope)\n\nfirst_level_copes_data = np.asarray(data) # (sessions x parcels x tpts_within_trial)\n\n# Create GLM data\nfirst_level_copes = glm.data.TrialGLMData(data=first_level_copes_data)\nprint(\"Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **NOTE**: Sign Ambiguity \nThere is a complication when comparing evoked responses across sessions and parcels (or dipoles, if we were working at the level of dipoles rather than parcels), caused by an ambiguity in the signs of the parcel time courses. In short, due to the way in which we source reconstruct and compute parcel time courses, we can not tell whether or not the values in a particular parcel and session should have their signs flipped. \n\nThis is a problem, for example, when:\n\n1) Pooling effects over sessions/subjects. This is because some sessions/subject may have their parcel time courses flipped one way, and other sessions/subjects their parcel time courses flipped the other way. Without solving this issue, averaging over sessions/subjects would not work.  \n\n2) Comparing an effect across parcels. This is because some parcels may have their time courses flipped one way, and other parcels have their time courses flipped the other way. Without solving this issue, spatial maps that show an effect as it changes over parcels might not look sensible.\n\nA solution we can use to solve this problem, is to rectify (take the absolute value) of the first-level COPE time courses. This has been carried out in the cell above by setting:\n``rectify = True``\n\nThis means that we will fit the group-level model to the **absolute value** of the first-level COPEs.\n\n### Fitting the Group Model\n\nWe can now fit the group-level GLM to the (sessions x parcels x tpts_within_trial) array, *data*, that contains the absolute value of the first-level COPEs. Essentially, the subject-wise, group-level design matrix will be fit separately to the first-level COPEs for every combination of parcels and timepoints-within-trial. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Fit Model\nprint(\"Fitting group-level GLM for first-level contrast {}\".format(first_level_contrast))\ngroup_model = glm.fit.OLSModel(design, first_level_copes)\nprint(\"Complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output and view COPE \n\nHere, we will output and view the parcel-wise COPEs for the first group-level contrast (index 0) of the first-level contrast specified above. \n\nWe first create a 3D niftii object in MNI space at a time point of interest. \n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from osl_ephys.source_recon import parcellation, rhino\nfrom nilearn import plotting\n\n# index for group contrast we want to output\ngroup_contrast_ind = group_model.copes.shape[0]-1 # group mean\n\n# time point of interest:\ntpt = 0.14 # in seconds\nvolume_num = np.abs(epochs_times-tpt).argmin() # finds index of nearest epoch time to tpt\n\n# The parcellation niftii file needs to be the same as was used to do the parcellation, \n# although it does not need to be at the same spatial resolution as the one used there.\nparcellation_file = 'HarvOxf-sub-Schaefer100-combined-2mm_4d.nii.gz'\nmask_file = \"MNI152_T1_2mm_brain.nii.gz\"\n\ncope_map = group_model.copes[group_contrast_ind, :, volume_num]\n\n# Create niftii object\nnii = parcellation.convert2niftii(cope_map, parcellation.find_file(parcellation_file), parcellation.find_file(mask_file))\n\nprint('Created 3D Niftii object for group contrast {}'.format(group_contrast_ind))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now view the 3D niftii object as a *png* image file, which shows the parcel-wise COPEs on the cortical surface.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Setup stats dir to put results into\nstats_dir = op.join(subjects_dir, \"glm_stats\")\nif not op.isdir(stats_dir):\n    os.makedirs(stats_dir, exist_ok=True)\n\ncope_fname = op.join(\n    stats_dir,\n    \"cope_gc{}_fc{}_vol{}\".format(group_contrast_ind, first_level_contrast, volume_num),\n)\n\nplotting.plot_img_on_surf(\n    nii,\n    views=[\"lateral\", \"medial\"],\n    hemispheres=[\"left\", \"right\"],\n    colorbar=True,\n    output_file=cope_fname,\n)\n\nos.system('open {}'.format(cope_fname + '.png'))\n\nprint(\"Complete\")\n\n\n# We can also create a 3D niftii file, which can then be viewed using *fsleyes*.\n\nimport nibabel as nib\n\nnib.save(nii, cope_fname + '.nii.gz')\nrhino.fsleyes([parcellation.find_file(mask_file), cope_fname + '.nii.gz'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In ``fsleyes``:\n\n* Set the positive colormap from ``Greyscale`` to ``Red-Yellow``\n* Turn on the negative colormap, and change it from ``Greyscale`` to ``Blue-LightBlue``\n* Set ``Min`` to 60, and ``Max`` to 150\n\n\n### Plot time course of group COPE \n\nLet's plot the group-averaged evoked response timecourse for a specified parcel in the visual cortex, alongside time courses for all sessions (19 subjects * 3 sessions per subject). The group-averaged evoked response timecourse corresponds to the group contrast with an index of 19, as defined earlier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parcel_ind = 53 # indexes from 0\n\nfirst_level_data = first_level_copes.data  # nsess x nparcels x ntpts\n\n# we divide the group mean COPE by nsubjects to get an average, \n# as the COPE was defined earlier as the sum over all subjects\ngroup_mean = group_model.copes[group_contrast_ind, parcel_ind, :].T/nsubjects  \n\nplt.figure()\nplt.plot(epochs_times, first_level_data[:, parcel_ind, :].T)\nplt.plot(epochs_times, group_mean, linewidth=2, color='k')\nplt.axvline(0, linestyle=\"--\", color=\"black\")\n\nplt.title(\n    \"abs(cope) for first-level contrast {}, parcel={}\".format(\n        first_level_contrast, parcel_ind\n    )\n)\nplt.xlabel(\"time (s)\")\nplt.ylabel(\"abs(cope)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A time of 0 secs corresponds to when the visual stimulus was presented. Each line in the plot corresponds to one of the ``3runs x 19subjects = 57sessions``, and shows the abs(COPE) timecourse from the chosen parcel for that session. This shows that there is a huge amount of between-session variability in the cope time course, around the group mean over subjects (which is shown as the black line). As a result, the black line does not look very much like a classic evoked response (ERP or ERF)!\n\nLet's now look to see how much of this variability is caused by between-subject differences by plotting each subject's mean timecourse. Note that each subject's mean timecourse is available as one of the group contrasts. \nWe will only plot a few subjects, to stop the plot becoming too cluttered:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import cm\n\nparcel_ind = 53 # indexes from 0, parcel in visual cortex\ntime_inds = np.where((epochs_times>-0.2) & (epochs_times<1.3))[0]\n\n# for better visualisation we will only plot the subject means for a few subjects\nsubjects2plot = [11,12,13,14,15]\n\n# get mean for each subject\nsubject_means = group_model.copes[:, parcel_ind, time_inds].T\nsubject_means = subject_means[:, subjects2plot]\n\n# compute standard deviation over sessions for each subject\nwithin_subject_stddev = np.sqrt(nsessions*group_model.varcopes[:, parcel_ind, time_inds]).T\nwithin_subject_stddev = within_subject_stddev[:, subjects2plot]\n\n# we divide the group mean COPE by nsubjects to get an average, \n# as the COPE was defined earlier as the sum over all subjects\ngroup_mean = group_model.copes[group_contrast_ind, parcel_ind, time_inds].T/nsubjects  \n\nclrs = cm.rainbow(np.linspace(0, 1, len(subjects2plot)))\n\nplt.figure()\n\nfor sub_ind in range(len(subjects2plot)):\n    plt.plot(epochs_times[time_inds], \n             subject_means[:, sub_ind],\n             c=clrs[sub_ind])\n    plt.fill_between(epochs_times[time_inds], \n                 subject_means[:, sub_ind]-within_subject_stddev[:, sub_ind], \n                 subject_means[:, sub_ind]+within_subject_stddev[:, sub_ind],\n                 alpha=0.3, \n                 facecolor=clrs[sub_ind])\n\nplt.plot(epochs_times[time_inds], group_mean, linewidth=2, color='k')\nplt.axvline(0, linestyle=\"--\", color=\"black\")\n\nplt.title(\n    \"abs(cope) for contrast {}, parcel={}\".format(\n        first_level_contrast, parcel_ind\n    )\n)\nplt.xlabel(\"time (s)\")\nplt.ylabel(\"abs(cope)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The group mean is again shown as the black line. Each subject's mean cope time course is shown plus/minus one stddev of the session population.\n\nThis shows that the overall between session variability is dominated by between subject variability, particularly in the timing of the peaks. While this issue is ameliorated by the use of the abs(cope), an alternative that helps further with this problem, is to do an analysis on the amplitude of the time course (e.g. computed using a Hilbert or Wavelet transform). Please see the \"Group Analysis on Amplitude Source-space Data\" tutorial for an example of this.\n\n### Statistics\n\nWhen we computed the first-level COPEs we subtracted the average baseline COPE value. This means that these baseline-corrected first-level COPEs are be expected to be zero if the activity is the same as the baseline period. This means that we can do a statistical test on the group mean COPE, for which any significant time points correspond to time points where the activity is different to the baseline period.\n\nWe will do a 2-tailed test, which finds where the group mean COPE is significantly larger, or smaller, than zero. \n\nWe will do this using permutation statistics on just the visual cortex parcel that we have already been looking at. We will also focus on a smaller time window to reduce unnecessarily excessive multiple comparison correction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's do permutation stats on a focussed time window to reduce excessive multiple comparison correction\nperm_time_inds = np.where((epochs_times>-0.1) & (epochs_times<0.8))[0]\nfirst_level_copes_data_4perms = first_level_copes_data.copy()\n\nfirst_level_copes_data_4perms = first_level_copes_data_4perms[:, parcel_ind, perm_time_inds]\nprint(\"data for stats is (subjects x timepoints) = {}\".format(first_level_copes_data_4perms.data.shape))\n\n# we divide the group mean COPE by nsubjects to get an average, \n# as the COPE was defined earlier as the sum over all subjects\nfirst_level_copes_4perms = glm.data.TrialGLMData(data=first_level_copes_data_4perms/nsubjects)\nperm = glm.permutations.MaxStatPermutation(design, \n                                           first_level_copes_4perms,         \n                                           contrast_idx=group_contrast_ind, # this is the group mean contrast\n                                           nperms=1000, \n                                           metric=\"copes\", \n                                           tail=0, # 2-tailed test\n                                           pooled_dims=1, # pool null distribution over time\n                                          )\nthres = perm.get_thresh(95)  # p-value=0.05\n\nprint(\"threshold:\", thres)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have a threshold for a p-value of 0.05, let's see which time points in the evoked response are significant.\nThese timepoints are shown by solid horizental black lines at the bottom of the plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parcel_ind = 53 # indexes from 0, parcel in visual cortex\ntime_inds = np.where((epochs_times>-0.2) & (epochs_times<1.3))[0]\n\n# we divide the group mean COPE by nsubjects to get an average, \n# as the COPE was defined earlier as the sum over all subjects\ngroup_mean = group_model.copes[group_contrast_ind, parcel_ind, time_inds].T/nsubjects\n\n#\u00a0Plot time points that are significant\nsignificant = (group_mean > thres) | (group_mean < -thres)\n\n# for better visualisation we will only plot a few subjects\nsubjects2plot = [11,12,13,14,15]\n\n# compute mean over sessions for each subject\nsubject_means = group_model.copes[:, parcel_ind, time_inds].T\nsubject_means = subject_means[:, subjects2plot]\n\n# compute standard deviation over sessions for each subject\nwithin_subject_stddev = np.sqrt(nsessions*group_model.varcopes[:, parcel_ind, time_inds]).T\nwithin_subject_stddev = within_subject_stddev[:, subjects2plot]\n\nclrs = cm.rainbow(np.linspace(0, 1, len(subjects2plot)))\n\nplt.figure()\n\nfor sub_ind in range(len(subjects2plot)):\n    plt.plot(epochs_times[time_inds], \n             subject_means[:, sub_ind],\n             c=clrs[sub_ind])\n    plt.fill_between(epochs_times[time_inds], \n                 subject_means[:, sub_ind]-within_subject_stddev[:, sub_ind], \n                 subject_means[:, sub_ind]+within_subject_stddev[:, sub_ind],\n                 alpha=0.3, \n                 facecolor=clrs[sub_ind])\n    \nplt.plot(epochs_times[time_inds], group_mean, linewidth=2, color='k')\n\nsig_times = epochs_times[time_inds][significant]\n\nif len(sig_times) > 0:\n    y = -5\n    plt.plot((sig_times.min(), sig_times.max()), (y, y), color='k', linewidth=4)    \n\nplt.axvline(0, linestyle=\"--\", color=\"black\")\nplt.title(\n    \"abs(cope) for contrast {}, parcel={}\".format(\n        first_level_contrast, parcel_ind\n    )\n)\nplt.xlabel(\"time (s)\")\nplt.ylabel(\"abs(cope)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### View group average as 4D niftii file \n\nEarlier, we viewed a 3D volume of the parcel-wise COPEs as a 3D niftii object at a time point of interest. \n\nWe will now view the parcel-wise COPEs over all timepoints within the trial, by outputting the parcel-wise COPEs as a 4D niftii object, where the 4th dimension is timepoint within trial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n\ncope_map = group_model.copes[group_contrast_ind, :, time_inds]\n\nnii = parcellation.convert2niftii(cope_map, \n                                  parcellation.find_file(parcellation_file), \n                                  parcellation.find_file(mask_file), \n                                  tres=epochs_times[1]-epochs_times[0], \n                                  tmin=epochs_times[time_inds[0]])\n\ncope_fname = op.join(\n    stats_dir,\n    \"cope_gc{}_fc{}\".format(group_contrast_ind, first_level_contrast, volume_num),\n)\n\n# Save cope as nii file and view in fsleyes\nprint(f\"Saving {cope_fname}\")\nnib.save(nii, cope_fname + '.nii.gz')\n\nparc_file_3d = 'HarvOxf-sub-Schaefer100-combined-2mm.nii.gz'\nrhino.fsleyes([parcellation.find_file(parc_file_3d), parcellation.find_file(mask_file), cope_fname + '.nii.gz'])\n\n\n\n\nparc_file_3d = 'HarvOxf-sub-Schaefer100-combined-2mm.nii.gz'\nrhino.fsleyes([parcellation.find_file(parc_file_3d), parcellation.find_file(mask_file), cope_fname + '.nii.gz'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In ``fsleyes``:\n\n* Change the positive colormap from *Greyscale* to *Red-Yellow*\n* Turn on the negative colormap, and change it from *Greyscale* to *Blue-LightBlue*\n* Set *Min* to 50, and *Max* to 150\n* Set *Volume* index to 52\n* Click on a voxel in the primary visual cortex\n* From the drop down menus Select *View/Time series*\n\nTo see the x-axis of the time series plots in secs, rather than by index:\n\n* In the Time series panel, select Settings (the spanner icon)\n* In the Time series settings popup, select \"Use Pix Dims\"\n\nFsleyes only shows time via the volume index (i.e. it is not in seconds) in the ortho-view.\nTo convert from volume index to time in seconds, or vice versa, use the following:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vol_index = 52 # indexes from 0\nprint('vol index of {}, corresponds to {} secs'.format(vol_index, epochs_times[time_inds[vol_index]]))\n\nt = 0.6 # secs\nvol_index = np.abs(epochs_times[time_inds] - t).argmin()\nprint('time of {}, corresponds to vol index of {}'.format(t, vol_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now view the parcel-wise COPEs over all timepoints within the trial, by outputting the parcel-wise COPEs as a 4D niftii object, where the 4th dimension is timepoint within trial.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}